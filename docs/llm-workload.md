# LLM推理负载特征分析

根据 `python examples/print_model_structure.py` 的信息，整理不同模型的推理负载特征对比。

## 模型配置对比

| 模型 | 参数量 | Hidden Size | Layers | Attention Heads | KV Heads | Attention Type |
|------|--------|-------------|--------|-----------------|----------|----------------|
| Qwen2.5-1.5B | 1.5B | 1,536 | 28 | 12 | 2 | GQA (6:1) |
| Qwen2.5-3B | 3B | 2,048 | 36 | 16 | 2 | GQA (8:1) |
| Qwen2.5-7B | 7B | 3,584 | 28 | 28 | 4 | GQA (7:1) |
| Llama-3.2-1B | 1B | 2,048 | 16 | 32 | 8 | GQA (4:1) |
| Llama2-7B | 7B | 4,096 | 32 | 32 | 32 | MHA |
| Llama2-70B | 70B | 8,192 | 80 | 64 | 8 | GQA (8:1) |

## 关键算子负载特征

### 1. 线性投影层 (Linear Projections)

| 算子 | Qwen2.5-1.5B | Qwen2.5-3B | Qwen2.5-7B | Llama-3.2-1B | Llama2-7B | Llama2-70B |
|------|--------------|------------|------------|--------------|-----------|------------|
| **Q_PROJ** | 2.36M | 4.19M | 12.85M | 4.19M | 16.78M | 67.11M |
| **K_PROJ** | 0.39M | 0.52M | 1.84M | 1.05M | 16.78M | 8.39M |
| **V_PROJ** | 0.39M | 0.52M | 1.84M | 1.05M | 16.78M | 8.39M |
| **O_PROJ** | 2.36M | 4.19M | 12.85M | 4.19M | 16.78M | 67.11M |

**特征分析：**
- **GQA优势**：K/V投影参数显著减少（相比MHA减少75-87%）
- **内存效率**：Qwen2.5-7B的K/V投影仅为Llama2-7B的11%
- **计算复杂度**：GQA在保持性能的同时大幅降低内存需求

### 2. 注意力计算 (Attention Computation)

| 算子 | Qwen2.5-1.5B | Qwen2.5-3B | Qwen2.5-7B | Llama-3.2-1B | Llama2-7B | Llama2-70B |
|------|--------------|------------|------------|--------------|-----------|------------|
| **QK^T MatMul** | [1,12,1024,1024] | [1,16,1024,1024] | [1,28,1024,1024] | [1,32,1024,1024] | [1,32,1024,1024] | [1,64,1024,1024] |
| **Softmax** | 12.58M | 16.78M | 29.36M | 33.55M | 33.55M | 67.11M |
| **AV MatMul** | [1,12,1024,128] | [1,16,1024,128] | [1,28,1024,128] | [1,32,1024,64] | [1,32,1024,128] | [1,64,1024,128] |

**特征分析：**
- **Softmax规模**：与attention heads数量成正比
- **内存复杂度**：O(seq_len²) per head
- **计算瓶颈**：QK^T矩阵乘法是主要计算瓶颈

### 3. 推理阶段负载特征

#### Prefill阶段 (初始提示处理)
| 模型 | 序列长度 | 注意力矩阵大小 | KV Cache/层 | 总KV Cache |
|------|----------|----------------|-------------|------------|
| Qwen2.5-1.5B | 1024 | [1,12,1024,1024] | 0.75MB | 21MB |
| Qwen2.5-3B | 1024 | [1,16,1024,1024] | 1.00MB | 36MB |
| Qwen2.5-7B | 1024 | [1,28,1024,1024] | 2.00MB | 56MB |
| Llama-3.2-1B | 1024 | [1,32,1024,1024] | 2.00MB | 32MB |
| Llama2-7B | 1024 | [1,32,1024,1024] | 2.00MB | 64MB |
| Llama2-70B | 1024 | [1,64,1024,1024] | 4.00MB | 320MB |

#### Decode阶段 (token生成)
| 模型 | 单token注意力 | KV增长/层 | 总KV增长 |
|------|---------------|-----------|----------|
| Qwen2.5-1.5B | [1,12,1,1024] | 0.0007MB | 0.02MB |
| Qwen2.5-3B | [1,16,1,1024] | 0.001MB | 0.036MB |
| Qwen2.5-7B | [1,28,1,1024] | 0.002MB | 0.056MB |
| Llama-3.2-1B | [1,32,1,1024] | 0.002MB | 0.032MB |
| Llama2-7B | [1,32,1,1024] | 0.002MB | 0.064MB |
| Llama2-70B | [1,64,1,1024] | 0.004MB | 0.32MB |

### 4. 内存带宽分析

| 模型 | Prefill内存访问 | Decode内存访问/token | 内存访问比例 |
|------|-----------------|---------------------|--------------|
| Qwen2.5-1.5B | 21MB | 0.02MB | 1050x |
| Qwen2.5-3B | 36MB | 0.04MB | 900x |
| Qwen2.5-7B | 56MB | 0.06MB | 933x |
| Llama-3.2-1B | 32MB | 0.03MB | 1067x |
| Llama2-7B | 64MB | 0.06MB | 1067x |
| Llama2-70B | 320MB | 0.32MB | 1000x |

## 关键发现

### 1. GQA vs MHA 效率对比
- **参数效率**：GQA相比MHA减少75-87%的K/V投影参数
- **内存效率**：KV Cache大小显著降低
- **性能保持**：在保持模型性能的同时大幅提升推理效率

**Llama-3.2-1B特殊优势：**
- **紧凑设计**：仅16层，相比Qwen2.5-3B的36层更高效
- **GQA优化**：32个attention heads，8个KV heads (4:1分组)
- **内存友好**：总KV Cache仅32MB，适合资源受限环境

### 2. 推理负载特征
- **Prefill阶段**：计算密集，内存访问量大
- **Decode阶段**：内存访问量小，但需要维护KV Cache
- **内存访问比例**：Prefill比Decode高1000倍以上

### 3. 硬件优化建议
- **TensorCore加速**：大型矩阵乘法（Q_PROJ, O_PROJ, MLP层）
- **向量加速**：Softmax, LayerNorm, 激活函数
- **内存优化**：KV Cache压缩，注意力稀疏化

## 完整算子负载特征

### Prefill阶段算子详情 (seq_len=1024)

| 算子 | Qwen2.5-1.5B | Qwen2.5-3B | Qwen2.5-7B | Llama-3.2-1B | Llama2-7B | Llama2-70B |
|------|--------------|------------|------------|--------------|-----------|------------|
| **Q_PROJ** | [1536]→[1536] | [2048]→[2048] | [3584]→[3584] | [2048]→[2048] | [4096]→[4096] | [8192]→[8192] |
| **K_PROJ** | [1536]→[256] | [2048]→[256] | [3584]→[512] | [2048]→[512] | [4096]→[4096] | [8192]→[1024] |
| **V_PROJ** | [1536]→[256] | [2048]→[256] | [3584]→[512] | [2048]→[512] | [4096]→[4096] | [8192]→[1024] |
| **QK^T** | [1,12,1024,128]×[1,2,1024,128]→[1,12,1024,1024] | [1,16,1024,128]×[1,2,1024,128]→[1,16,1024,1024] | [1,28,1024,128]×[1,4,1024,128]→[1,28,1024,1024] | [1,32,1024,64]×[1,8,1024,64]→[1,32,1024,1024] | [1,32,1024,128]×[1,32,1024,128]→[1,32,1024,1024] | [1,64,1024,128]×[1,8,1024,128]→[1,64,1024,1024] |
| **Softmax** | [1,12,1024,1024] | [1,16,1024,1024] | [1,28,1024,1024] | [1,32,1024,1024] | [1,32,1024,1024] | [1,64,1024,1024] |
| **AV** | [1,12,1024,1024]×[1,2,1024,128]→[1,12,1024,128] | [1,16,1024,1024]×[1,2,1024,128]→[1,16,1024,128] | [1,28,1024,1024]×[1,4,1024,128]→[1,28,1024,128] | [1,32,1024,1024]×[1,8,1024,64]→[1,32,1024,64] | [1,32,1024,1024]×[1,32,1024,128]→[1,32,1024,128] | [1,64,1024,1024]×[1,8,1024,128]→[1,64,1024,128] |
| **O_PROJ** | [1536]→[1536] | [2048]→[2048] | [3584]→[3584] | [2048]→[2048] | [4096]→[4096] | [8192]→[8192] |
| **Gate_Proj** | [1536]→[6144] | [2048]→[8192] | [3584]→[18944] | [2048]→[8192] | [4096]→[11008] | [8192]→[22016] |
| **Up_Proj** | [1536]→[6144] | [2048]→[8192] | [3584]→[18944] | [2048]→[8192] | [4096]→[11008] | [8192]→[22016] |
| **SiLU** | [6144] | [8192] | [18944] | [8192] | [11008] | [22016] |
| **Mul** | [6144]×[6144]→[6144] | [8192]×[8192]→[8192] | [18944]×[18944]→[18944] | [8192]×[8192]→[8192] | [11008]×[11008]→[11008] | [22016]×[22016]→[22016] |
| **Down_Proj** | [6144]→[1536] | [8192]→[2048] | [18944]→[3584] | [8192]→[2048] | [11008]→[4096] | [22016]→[8192] |
| **RMSNorm** | [1536] | [2048] | [3584] | [2048] | [4096] | [8192] |
| **ResidualAdd** | [1536]+[1536]→[1536] | [2048]+[2048]→[2048] | [3584]+[3584]→[3584] | [2048]+[2048]→[2048] | [4096]+[4096]→[4096] | [8192]+[8192]→[8192] |

### Decode阶段算子详情 (seq_len=1, 累积KV cache)

| 算子 | Qwen2.5-1.5B | Qwen2.5-3B | Qwen2.5-7B | Llama-3.2-1B | Llama2-7B | Llama2-70B |
|------|--------------|------------|------------|--------------|-----------|------------|
| **Q_PROJ** | [1536]→[1536] | [2048]→[2048] | [3584]→[3584] | [2048]→[2048] | [4096]→[4096] | [8192]→[8192] |
| **K_PROJ** | [1536]→[256] | [2048]→[256] | [3584]→[512] | [2048]→[512] | [4096]→[4096] | [8192]→[1024] |
| **V_PROJ** | [1536]→[256] | [2048]→[256] | [3584]→[512] | [2048]→[512] | [4096]→[4096] | [8192]→[1024] |
| **QK^T** | [1,12,1,128]×[1,2,1024,128]→[1,12,1,1024] | [1,16,1,128]×[1,2,1024,128]→[1,16,1,1024] | [1,28,1,128]×[1,4,1024,128]→[1,28,1,1024] | [1,32,1,64]×[1,8,1024,64]→[1,32,1,1024] | [1,32,1,128]×[1,32,1024,128]→[1,32,1,1024] | [1,64,1,128]×[1,8,1024,128]→[1,64,1,1024] |
| **Softmax** | [1,12,1,1024] | [1,16,1,1024] | [1,28,1,1024] | [1,32,1,1024] | [1,32,1,1024] | [1,64,1,1024] |
| **AV** | [1,12,1,1024]×[1,2,1024,128]→[1,12,1,128] | [1,16,1,1024]×[1,2,1024,128]→[1,16,1,128] | [1,28,1,1024]×[1,4,1024,128]→[1,28,1,128] | [1,32,1,1024]×[1,8,1024,64]→[1,32,1,64] | [1,32,1,1024]×[1,32,1024,128]→[1,32,1,128] | [1,64,1,1024]×[1,8,1024,128]→[1,64,1,128] |
| **O_PROJ** | [1536]→[1536] | [2048]→[2048] | [3584]→[3584] | [2048]→[2048] | [4096]→[4096] | [8192]→[8192] |
| **Gate_Proj** | [1536]→[6144] | [2048]→[8192] | [3584]→[18944] | [2048]→[8192] | [4096]→[11008] | [8192]→[22016] |
| **Up_Proj** | [1536]→[6144] | [2048]→[8192] | [3584]→[18944] | [2048]→[8192] | [4096]→[11008] | [8192]→[22016] |
| **SiLU** | [6144] | [8192] | [18944] | [8192] | [11008] | [22016] |
| **Mul** | [6144]×[6144]→[6144] | [8192]×[8192]→[8192] | [18944]×[18944]→[18944] | [8192]×[8192]→[8192] | [11008]×[11008]→[11008] | [22016]×[22016]→[22016] |
| **Down_Proj** | [6144]→[1536] | [8192]→[2048] | [18944]→[3584] | [8192]→[2048] | [11008]→[4096] | [22016]→[8192] |
| **RMSNorm** | [1536] | [2048] | [3584] | [2048] | [4096] | [8192] |
| **ResidualAdd** | [1536]+[1536]→[1536] | [2048]+[2048]→[2048] | [3584]+[3584]→[3584] | [2048]+[2048]→[2048] | [4096]+[4096]→[4096] | [8192]+[8192]→[8192] |

### 算子特征分析

**注意力层算子：**
- **Q_PROJ/O_PROJ**：维度最大，适合TensorCore加速
- **K_PROJ/V_PROJ**：GQA模型显著减少参数，内存效率高
- **QK^T**：Prefill阶段计算密集，Decode阶段内存访问少
- **Softmax**：适合向量加速，维度与attention heads相关

**FFN层算子：**
- **Gate_Proj/Up_Proj**：大型矩阵乘法，TensorCore加速
- **SiLU**：激活函数，向量加速
- **Mul**：元素级乘法，向量加速
- **Down_Proj**：大型矩阵乘法，TensorCore加速

**其他算子：**
- **RMSNorm**：LayerNorm变体，向量加速
- **ResidualAdd**：残差连接，向量加速

## 数据来源
基于 `python examples/print_model_structure.py` 分析结果，配置：batch_size=1, input_seq_len=1024, output_seq_len=128, torch_dtype=float16
